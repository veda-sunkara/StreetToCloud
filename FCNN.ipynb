{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7db1ba0d1b3f46a480cb8ab4cc9aab71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1db6ecfb72054e4eb7288600fbc46f62",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_53ea9700fa0e41b9a39d94f383aa6252",
              "IPY_MODEL_7c40911347034a9b94d43bc8c62673f7"
            ]
          }
        },
        "1db6ecfb72054e4eb7288600fbc46f62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53ea9700fa0e41b9a39d94f383aa6252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d66b8e660df84820af4806d4991e68d1",
            "_dom_classes": [],
            "description": " 63%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 90,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 57,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c8c5213ce8f4b5aa3796eee88cc11f1"
          }
        },
        "7c40911347034a9b94d43bc8c62673f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_784fd88d1c6d457685c8353f48e58041",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 57/90 [00:08&lt;00:04,  7.37it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efdc15e854f744ec9a6fa7abd5fffd45"
          }
        },
        "d66b8e660df84820af4806d4991e68d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c8c5213ce8f4b5aa3796eee88cc11f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "784fd88d1c6d457685c8353f48e58041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efdc15e854f744ec9a6fa7abd5fffd45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lI3CYy7RsSu",
        "outputId": "e884f42e-b6bb-41fa-a4c2-ac2278c4e82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!sudo mkdir files\n",
        "!sudo mkdir files/S1\n",
        "!sudo mkdir files/QC_v2\n",
        "!sudo mkdir files/QC_v2_shrunk\n",
        "!gsutil -m rsync -r gs://cnn_chips/S1 files/S1\n",
        "#!gsutil -m rsync -r gs://refined_cnn_chips/QC_v2_shrunk files/QC_v2_shrunk\n",
        "!gsutil -m rsync -r gs://cnn_chips/QC_v2 files/QC_v2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘files’: File exists\n",
            "mkdir: cannot create directory ‘files/S1’: File exists\n",
            "mkdir: cannot create directory ‘files/QC_v2’: File exists\n",
            "mkdir: cannot create directory ‘files/QC_v2_shrunk’: File exists\n",
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Building synchronization state...\n",
            "Starting synchronization...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwLbvKsGE_Qm",
        "outputId": "84a52f69-d027-408a-9303-1c5203f433cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!curl https://sdk.cloud.google.com | bash\n",
        "\n",
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   443  100   443    0     0  31642      0 --:--:-- --:--:-- --:--:-- 34076\n",
            "Downloading Google Cloud SDK install script: https://dl.google.com/dl/cloudsdk/channels/rapid/install_google_cloud_sdk.bash\n",
            "\r######################################################################## 100.0%\n",
            "Running install script from: /tmp/tmp.3apzS1z16E/install_google_cloud_sdk.bash\n",
            "which curl\n",
            "curl -# -f https://dl.google.com/dl/cloudsdk/channels/rapid/google-cloud-sdk.tar.gz\n",
            "\r######################################################################## 100.0%\n",
            "\n",
            "mkdir -p /root\n",
            "\"/root/google-cloud-sdk\" already exists and may contain out of date files.\n",
            "Remove /root/google-cloud-sdk or select a new installation directory, then run again.\n",
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [default] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  gce_metadata_read_timeout_sec: '0'\n",
            "core:\n",
            "  account: matthewjosephpurri@gmail.com\n",
            "  project: singular-skill-279200\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [default] with new settings \n",
            " [2] Create a new configuration\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [default]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "Choose the account you would like to use to perform operations for \n",
            "this configuration:\n",
            " [1] matthewjosephpurri@gmail.com\n",
            " [2] Log in with a new account\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "You are logged in as: [matthewjosephpurri@gmail.com].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] singular-skill-279200\n",
            " [2] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  1\n",
            "\n",
            "Your current project has been set to: [singular-skill-279200].\n",
            "\n",
            "Not setting default zone/region (this feature makes it easier to use\n",
            "[gcloud compute] by setting an appropriate default value for the\n",
            "--zone and --region flag).\n",
            "See https://cloud.google.com/compute/docs/gcloud-compute section on how to set\n",
            "default compute region and zone manually. If you would like [gcloud init] to be\n",
            "able to do this for you the next time you run it, make sure the\n",
            "Compute Engine API is enabled for your project on the\n",
            "https://console.developers.google.com/apis page.\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use matthewjosephpurri@gmail.com by default\n",
            "* Commands will reference project `singular-skill-279200` by default\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwpVipZNScc8",
        "outputId": "e3e1b589-54dd-4476-89fd-1630d1063d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   653  100   653    0     0  19787      0 --:--:-- --:--:-- --:--:-- 19787\n",
            "OK\n",
            "53 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "gcsfuse is already the newest version (0.30.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "265yaPPqfMBO",
        "outputId": "3041f498-3f63-41f7-eb29-04e5f36937eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cd /home\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIBBqZanOGab",
        "outputId": "aafb1578-c4c6-4799-cb22-66e670fc9c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip install rasterio\n",
        "\n",
        "!gsutil cp gs://cnn_chips/flood_test_data.csv .\n",
        "!gsutil cp gs://cnn_chips/flood_train_data.csv .\n",
        "!gsutil cp gs://cnn_chips/flood_valid_data.csv .\n",
        "!gsutil cp gs://cnn_chips/shrunk_flood_test_data.csv ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.6/dist-packages (1.1.6)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.6/dist-packages (from rasterio) (2.3.0)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from rasterio) (0.5.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.18.5)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (20.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
            "Copying gs://cnn_chips/flood_test_data.csv...\n",
            "/ [1 files][  4.5 KiB/  4.5 KiB]                                                \n",
            "Operation completed over 1 objects/4.5 KiB.                                      \n",
            "Copying gs://cnn_chips/flood_train_data.csv...\n",
            "/ [1 files][ 12.5 KiB/ 12.5 KiB]                                                \n",
            "Operation completed over 1 objects/12.5 KiB.                                     \n",
            "Copying gs://cnn_chips/flood_valid_data.csv...\n",
            "/ [1 files][  4.4 KiB/  4.4 KiB]                                                \n",
            "Operation completed over 1 objects/4.4 KiB.                                      \n",
            "CommandException: No URLs matched: gs://cnn_chips/shrunk_flood_test_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q32qwjX4s6ml"
      },
      "source": [
        "LR = 1e-3\n",
        "EPOCHS = 1000\n",
        "EPOCHS_PER_UPDATE = 1\n",
        "RUNNAME = \"1e3_flood_0\"\n",
        "BASEDIR = ''\n",
        "\n",
        "MODEL_NAME = 'refiner'  # [unet, fcn, refiner]\n",
        "CS_POINTS_CLUSTERING = None  # [None, low, high]\n",
        "CS_POINTS_NOISE = None  # [None, low, high]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgxaTLSm0Hpb"
      },
      "source": [
        "# get crowd source points data path\n",
        "def get_cs_points_path(base_dir, cluster_type, noise_type):\n",
        "    try:\n",
        "        cluster_type = cluster_type.lower()\n",
        "        noise_type = noise_type.lower()\n",
        "    except AttributeError:\n",
        "        # either cluster_type or noise_type is None\n",
        "        return None\n",
        "\n",
        "    if not cluster_type in ['low', 'high']:\n",
        "        print('FATAL: Invalid cluster type {}'.format(cluster_type))\n",
        "        exit()\n",
        "\n",
        "    if not noise_type in ['low', 'high']:\n",
        "        print('FATAL: Invalid noise type {}'.format(noise_type))\n",
        "        exit()\n",
        "\n",
        "    cs_points_path = os.path.join(base_dir, 'QC_v2_cluster_{}_noise_{}.p'.format(cluster_type, noise_type))\n",
        "\n",
        "    return cs_points_path\n",
        "\n",
        "crowd_points_path = get_cs_points_path(BASEDIR, CS_POINTS_CLUSTERING, CS_POINTS_NOISE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXOFDO5QM9L2"
      },
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "class InMemoryDataset(torch.utils.data.Dataset):\n",
        "  \n",
        "  def __init__(self, data_list, preprocess_func):\n",
        "    self.data_list = data_list\n",
        "    self.preprocess_func = preprocess_func    \n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    return self.preprocess_func(self.data_list[i])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data_list)\n",
        "\n",
        "\n",
        "def processAndAugment(data):\n",
        "  output = {}\n",
        "  try:\n",
        "    (x,y) = data\n",
        "    p = None\n",
        "  except:\n",
        "    x,y,p = data  # image, mask, point image\n",
        "    p = Image.fromarray(p)\n",
        "  im, label = x.copy(), y.copy()\n",
        "\n",
        "  # convert to PIL for easier transforms\n",
        "  im1 = Image.fromarray(im[0])\n",
        "  im2 = Image.fromarray(im[1])\n",
        "  label = Image.fromarray(label.squeeze())\n",
        "\n",
        "  # Get params for random transforms\n",
        "  i, j, h, w = transforms.RandomCrop.get_params(im1, (256, 256))\n",
        "  \n",
        "  im1 = F.crop(im1, i, j, h, w)\n",
        "  im2 = F.crop(im2, i, j, h, w)\n",
        "  label = F.crop(label, i, j, h, w)\n",
        "  if not p is None:\n",
        "    p = F.crop(p, i, j, h, w)\n",
        "\n",
        "  if random.random() > 0.5:\n",
        "    im1 = F.hflip(im1)\n",
        "    im2 = F.hflip(im2)\n",
        "    label = F.hflip(label)\n",
        "    if not p is None:\n",
        "      p = F.hflip(p)\n",
        "\n",
        "  if random.random() > 0.5:\n",
        "    im1 = F.vflip(im1)\n",
        "    im2 = F.vflip(im2)\n",
        "    label = F.vflip(label)\n",
        "    if not p is None:\n",
        "      p = F.vflip(p)\n",
        "\n",
        "  norm = transforms.Normalize([0.6851, 0.5235], [0.0820, 0.1102])\n",
        "  im = torch.stack([transforms.ToTensor()(im1).squeeze(), transforms.ToTensor()(im2).squeeze()])\n",
        "  im = norm(im)\n",
        "  label = transforms.ToTensor()(label).squeeze()\n",
        "  if torch.sum(label.gt(.003) * label.lt(.004)):\n",
        "    label *= 255\n",
        "  label = label.round()\n",
        "\n",
        "  output['image'] = im\n",
        "  output['label'] = label\n",
        "\n",
        "  if not p is None:\n",
        "    p = F.to_tensor(p)\n",
        "    output['point_img'] = p\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "def processTestIm(data):\n",
        "  output = {}\n",
        "  try:\n",
        "    (x,y) = data\n",
        "    p = None\n",
        "  except:\n",
        "    x,y,p = data  # image, mask, point image\n",
        "    p = Image.fromarray(p)\n",
        "\n",
        "  im, label = x.copy(), y.copy()\n",
        "\n",
        "  norm = transforms.Normalize([0.6851, 0.5235], [0.0820, 0.1102])\n",
        "  #label[0][0][0] = 255\n",
        "  \n",
        "  # convert to PIL for easier transforms\n",
        "  im_c1 = Image.fromarray(im[0]).resize((512,512))\n",
        "  im_c2 = Image.fromarray(im[1]).resize((512,512))\n",
        "  label = Image.fromarray(label.squeeze()).resize((512,512))\n",
        "\n",
        "  im_c1s = [F.crop(im_c1, 0, 0, 256, 256), F.crop(im_c1, 0, 256, 256, 256),\n",
        "            F.crop(im_c1, 256, 0, 256, 256), F.crop(im_c1, 256, 256, 256, 256)]\n",
        "  im_c2s = [F.crop(im_c2, 0, 0, 256, 256), F.crop(im_c2, 0, 256, 256, 256),\n",
        "            F.crop(im_c2, 256, 0, 256, 256), F.crop(im_c2, 256, 256, 256, 256)]\n",
        "  labels = [F.crop(label, 0, 0, 256, 256), F.crop(label, 0, 256, 256, 256),\n",
        "            F.crop(label, 256, 0, 256, 256), F.crop(label, 256, 256, 256, 256)]\n",
        "  if not p is None:\n",
        "    p = [F.crop(p, 0, 0, 256, 256), F.crop(p, 0, 256, 256, 256),\n",
        "         F.crop(p, 256, 0, 256, 256), F.crop(p, 256, 256, 256, 256)]\n",
        "    p = [F.to_tensor(p_img) for p_img in p]\n",
        "    p = torch.stack(p)\n",
        "\n",
        "  ims = [torch.stack((transforms.ToTensor()(x).squeeze(),\n",
        "                    transforms.ToTensor()(y).squeeze()))\n",
        "                    for (x,y) in zip(im_c1s, im_c2s)]\n",
        "  ims = [norm(im) for im in ims]\n",
        "  ims = torch.stack(ims)\n",
        "  labels = [(transforms.ToTensor()(label).squeeze()) for label in labels]\n",
        "  labels = torch.stack(labels)\n",
        "  if torch.sum(labels.gt(.003) * labels.lt(.004)):\n",
        "    labels *= 255\n",
        "  labels = labels.round()\n",
        "\n",
        "  output['image'] = ims\n",
        "  output['label'] = labels\n",
        "\n",
        "  if not p is None:\n",
        "    output['point_img'] = p\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PxA9idPPKJ1"
      },
      "source": [
        "import csv\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def getArr(fname):\n",
        "  return rasterio.open('files/' + fname).read()\n",
        "\n",
        "\n",
        "def download_perm_water_data_from_file(fname, crowd_points_path=None):\n",
        "  if not crowd_points_path is None:\n",
        "    try:\n",
        "      crowd_points_dict = pickle.load(open(crowd_points_path, 'rb'))\n",
        "    except IOError:\n",
        "      print('Cant access crowd source points, using fake points.')\n",
        "      crowd_points_dict = {}\n",
        "  with open(fname) as f:\n",
        "    data_fnames = [tuple(line) for line in csv.reader(f)]\n",
        "  i = 0\n",
        "  data = []\n",
        "  for (x,y) in data_fnames:\n",
        "    # print(x, y)\n",
        "    arr_x, arr_y = getArr(x), getArr(y)\n",
        "    if np.sum((arr_x != arr_x)) == 0:\n",
        "      ignore = (arr_y == -1)\n",
        "      ignore = ((np.uint8(ignore) * -1) * 256) + 1\n",
        "      arr_y *= ignore\n",
        "      if not crowd_points_path is None:\n",
        "        # get image name\n",
        "        img_name = os.path.splitext(os.path.split(x)[1])[0]\n",
        "        try:\n",
        "          point_image = crowd_points_dict[img_name]\n",
        "        except KeyError:\n",
        "          point_image = np.zeros((512, 512))\n",
        "        data.append((arr_x, arr_y, point_image))\n",
        "      else:\n",
        "        data.append((arr_x, arr_y))\n",
        "      i+=1\n",
        "      print(i)\n",
        "    else:\n",
        "      print(\"skipping nan\")\n",
        "  return data\n",
        "\n",
        "def download_perm_train_data():\n",
        "  TRAINING_DATA_FILE = BASEDIR + 'flood_train_data.csv'\n",
        "  return download_perm_water_data_from_file(TRAINING_DATA_FILE, crowd_points_path)\n",
        "\n",
        "def download_perm_valid_data():\n",
        "  VALID_DATA_FILE = BASEDIR + 'flood_valid_data.csv'\n",
        "  return download_perm_water_data_from_file(VALID_DATA_FILE, crowd_points_path)\n",
        "\n",
        "def download_perm_test_data():\n",
        "  TEST_DATA_FILE = BASEDIR + 'flood_test_data.csv'\n",
        "  return download_perm_water_data_from_file(TEST_DATA_FILE, crowd_points_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtBqkzsLSm3x"
      },
      "source": [
        "from time import time\n",
        "\n",
        "def getArrFlood(fname):\n",
        "  return rasterio.open(fname).read()\n",
        "\n",
        "def download_flood_water_data_from_list(l, crowd_points_path=None):\n",
        "  if not crowd_points_path is None:\n",
        "    try:\n",
        "      crowd_points_dict = pickle.load(open(crowd_points_path, 'rb'))\n",
        "    except IOError:\n",
        "      print('Cant access crowd source points, using fake points.')\n",
        "      crowd_points_dict = {}\n",
        "  i= 0\n",
        "  tot_nan = 0\n",
        "  tot_good = 0\n",
        "  flood_data = []\n",
        "  for (im_fname, mask_fname) in l:\n",
        "    print(im_fname)\n",
        "    if not os.path.exists(os.path.join(\"files/\", im_fname)):\n",
        "      print(os.path.join(\"files/\", im_fname))\n",
        "      continue\n",
        "    arr_x = np.nan_to_num(getArrFlood(os.path.join(\"files/\", im_fname)))\n",
        "    arr_y = getArrFlood(os.path.join(\"files/\", mask_fname))\n",
        "    ignore = (arr_y == -1)\n",
        "    ignore = ((np.uint8(ignore) * -1) * 256) + 1\n",
        "    # arr_y *= ignore\n",
        "    arr_y = np.uint8(getArrFlood(os.path.join(\"files/\", mask_fname)))\n",
        "    if np.sum((arr_y != arr_y)) == 0:\n",
        "      arr_x = np.clip(arr_x, -50, 1)\n",
        "      arr_x = (arr_x + 50) / 51\n",
        "      if i % 100 == 0:\n",
        "        print(i)\n",
        "        print(im_fname, mask_fname)\n",
        "      i += 1\n",
        "      if not crowd_points_path is None:\n",
        "        # get image name\n",
        "        img_name = os.path.splitext(os.path.split(im_fname)[1])[0]\n",
        "        try:\n",
        "          point_image = crowd_points_dict[img_name]\n",
        "        except KeyError:\n",
        "          point_image = np.zeros((512, 512))\n",
        "        flood_data.append((arr_x, arr_y, point_image))\n",
        "      else:\n",
        "        flood_data.append((arr_x, arr_y))\n",
        "    else:\n",
        "      print(\"skipping nan\")\n",
        "  print(i)\n",
        "  return flood_data\n",
        "\n",
        "def load_flood_train_data():\n",
        "  basedir = \"\"\n",
        "  fname = \"flood_train_data.csv\"\n",
        "  with open(fname) as f:\n",
        "    fname = [tuple(line) for line in csv.reader(f)]\n",
        "  return download_flood_water_data_from_list(fname, crowd_points_path)\n",
        "\n",
        "def load_weak_flood_train_data():\n",
        "  basedir = \"\"\n",
        "  files = [(os.path.join(\"S1_NoQC\", x[1]), os.path.join(\"NoQC\", x[0])) for x in zip(sorted(os.listdir(\"files/NoQC\")), sorted(os.listdir(\"files/S1_NoQC\")))]\n",
        "  files = [x for x in files if \"Bolivia\" not in x[0]]\n",
        "  print(files[0:10])\n",
        "  return download_flood_water_data_from_list(files, crowd_points_path)\n",
        "\n",
        "def load_flood_test_perm_data():\n",
        "  fname = \"flood_test_data.csv\"\n",
        "  with open(fname) as f:\n",
        "    fname = [tuple(line) for line in csv.reader(f)]\n",
        "  name = [(t[0], t[1].replace(\"QC_v2\", \"Perm\").replace(\"QC\", \"Perm\")) for t in fname]\n",
        "  return download_flood_water_data_from_list(fname, crowd_points_path)\n",
        "\n",
        "def load_flood_valid_data():\n",
        "  basedir = \"\"\n",
        "  fname = \"flood_valid_data.csv\"\n",
        "  with open(fname) as f:\n",
        "    fname = [tuple(line) for line in csv.reader(f)]\n",
        "  print(fname, \"files!\")\n",
        "  return download_flood_water_data_from_list(fname, crowd_points_path)\n",
        "\n",
        "def load_flood_test_data():\n",
        "  basedir = \"\"\n",
        "  fname = \"flood_test_data.csv\"\n",
        "  with open(fname) as f:\n",
        "    fname = [tuple(line) for line in csv.reader(f)]\n",
        "  return download_flood_water_data_from_list(fname, crowd_points_path)\n",
        "\n",
        "def load_flood_bolivia_test_data():\n",
        "  basedir = \"\"\n",
        "  fname = \"flood_bolivia_data.csv\"\n",
        "  with open(fname) as f:\n",
        "    fname = [tuple(line) for line in csv.reader(f)]\n",
        "  return download_flood_water_data_from_list(fname, crowd_points_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3RucO3iTk6W",
        "outputId": "9537984c-dd3b-4ae5-f6c5-84c9d5767550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!ls\n",
        "train_data = download_perm_train_data()\n",
        "train_dataset = InMemoryDataset(train_data, processAndAugment)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, sampler=None,\n",
        "                  batch_sampler=None, num_workers=0, collate_fn=None,\n",
        "                  pin_memory=True, drop_last=False, timeout=0,\n",
        "                  worker_init_fn=None)\n",
        "train_iter = iter(train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json     files\t\t  flood_train_data.csv\tsample_data\n",
            "checkpoints  flood_test_data.csv  flood_valid_data.csv\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "skipping nan\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "skipping nan\n",
            "15\n",
            "skipping nan\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "skipping nan\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "skipping nan\n",
            "27\n",
            "28\n",
            "29\n",
            "skipping nan\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "skipping nan\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "skipping nan\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "skipping nan\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "skipping nan\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "skipping nan\n",
            "64\n",
            "65\n",
            "66\n",
            "skipping nan\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "skipping nan\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "skipping nan\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "skipping nan\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "skipping nan\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "skipping nan\n",
            "skipping nan\n",
            "203\n",
            "204\n",
            "205\n",
            "skipping nan\n",
            "206\n",
            "207\n",
            "208\n",
            "skipping nan\n",
            "209\n",
            "skipping nan\n",
            "210\n",
            "211\n",
            "212\n",
            "skipping nan\n",
            "213\n",
            "214\n",
            "215\n",
            "skipping nan\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKjI0-zSOJCJ",
        "outputId": "e0720a35-9376-499d-a41c-bf6f32a4ed6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mkdir checkpoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘checkpoints’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcOUaISHuwUx",
        "outputId": "dd923618-d7a6-4f8b-bed4-9ccfee3623ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "flood_test_perm_data = load_flood_test_perm_data()\n",
        "flood_test_all_data = load_flood_test_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "S1/Ghana_313799_S1.tif\n",
            "0\n",
            "S1/Ghana_313799_S1.tif QC_v2/Ghana_313799_QC.tif\n",
            "S1/Ghana_1078550_S1.tif\n",
            "S1/Ghana_97059_S1.tif\n",
            "S1/Ghana_359826_S1.tif\n",
            "S1/Ghana_319168_S1.tif\n",
            "S1/Ghana_866994_S1.tif\n",
            "S1/Ghana_406026_S1.tif\n",
            "S1/Ghana_53713_S1.tif\n",
            "S1/Ghana_83483_S1.tif\n",
            "S1/Ghana_167233_S1.tif\n",
            "S1/Ghana_141271_S1.tif\n",
            "S1/India_900498_S1.tif\n",
            "S1/India_591317_S1.tif\n",
            "S1/India_747992_S1.tif\n",
            "S1/India_79637_S1.tif\n",
            "S1/India_952728_S1.tif\n",
            "S1/India_828067_S1.tif\n",
            "S1/India_570384_S1.tif\n",
            "S1/India_44475_S1.tif\n",
            "S1/India_80221_S1.tif\n",
            "S1/India_1018327_S1.tif\n",
            "S1/India_592446_S1.tif\n",
            "S1/India_772630_S1.tif\n",
            "S1/India_631692_S1.tif\n",
            "S1/India_399883_S1.tif\n",
            "S1/Mekong_333434_S1.tif\n",
            "S1/Mekong_45934_S1.tif\n",
            "S1/Mekong_1443339_S1.tif\n",
            "S1/Mekong_382276_S1.tif\n",
            "S1/Mekong_254910_S1.tif\n",
            "S1/Mekong_424793_S1.tif\n",
            "S1/Nigeria_417184_S1.tif\n",
            "S1/Nigeria_225131_S1.tif\n",
            "S1/Nigeria_812045_S1.tif\n",
            "S1/Nigeria_22088_S1.tif\n",
            "S1/Pakistan_849790_S1.tif\n",
            "S1/Pakistan_664885_S1.tif\n",
            "S1/Pakistan_694942_S1.tif\n",
            "S1/Pakistan_70625_S1.tif\n",
            "S1/Pakistan_528249_S1.tif\n",
            "S1/Pakistan_167553_S1.tif\n",
            "S1/Paraguay_913449_S1.tif\n",
            "S1/Paraguay_280900_S1.tif\n",
            "S1/Paraguay_790830_S1.tif\n",
            "S1/Paraguay_232281_S1.tif\n",
            "S1/Paraguay_271769_S1.tif\n",
            "S1/Paraguay_40936_S1.tif\n",
            "S1/Paraguay_1029191_S1.tif\n",
            "S1/Paraguay_59731_S1.tif\n",
            "S1/Paraguay_683296_S1.tif\n",
            "S1/Paraguay_34417_S1.tif\n",
            "S1/Paraguay_868895_S1.tif\n",
            "S1/Paraguay_511199_S1.tif\n",
            "S1/Paraguay_80102_S1.tif\n",
            "S1/Paraguay_651904_S1.tif\n",
            "S1/Somalia_699062_S1.tif\n",
            "S1/Somalia_60129_S1.tif\n",
            "S1/Somalia_685158_S1.tif\n",
            "S1/Somalia_166342_S1.tif\n",
            "S1/Somalia_94102_S1.tif\n",
            "S1/Somalia_322855_S1.tif\n",
            "S1/Spain_6860600_S1.tif\n",
            "S1/Spain_7558720_S1.tif\n",
            "S1/Spain_5650136_S1.tif\n",
            "S1/Spain_7370579_S1.tif\n",
            "S1/Spain_6095801_S1.tif\n",
            "S1/Spain_7387658_S1.tif\n",
            "S1/Sri-Lanka_117737_S1.tif\n",
            "S1/Sri-Lanka_1049830_S1.tif\n",
            "S1/Sri-Lanka_534068_S1.tif\n",
            "S1/Sri-Lanka_922192_S1.tif\n",
            "S1/Sri-Lanka_377277_S1.tif\n",
            "S1/Sri-Lanka_649970_S1.tif\n",
            "S1/Sri-Lanka_450918_S1.tif\n",
            "S1/Sri-Lanka_849649_S1.tif\n",
            "S1/Sri-Lanka_713926_S1.tif\n",
            "S1/USA_905409_S1.tif\n",
            "S1/USA_430764_S1.tif\n",
            "S1/USA_350244_S1.tif\n",
            "S1/USA_527077_S1.tif\n",
            "S1/USA_933610_S1.tif\n",
            "S1/USA_66511_S1.tif\n",
            "S1/USA_519181_S1.tif\n",
            "S1/USA_778194_S1.tif\n",
            "S1/USA_67102_S1.tif\n",
            "S1/USA_1049586_S1.tif\n",
            "S1/USA_595451_S1.tif\n",
            "S1/USA_670826_S1.tif\n",
            "S1/USA_504150_S1.tif\n",
            "S1/USA_758178_S1.tif\n",
            "90\n",
            "S1/Ghana_313799_S1.tif\n",
            "0\n",
            "S1/Ghana_313799_S1.tif QC_v2/Ghana_313799_QC.tif\n",
            "S1/Ghana_1078550_S1.tif\n",
            "S1/Ghana_97059_S1.tif\n",
            "S1/Ghana_359826_S1.tif\n",
            "S1/Ghana_319168_S1.tif\n",
            "S1/Ghana_866994_S1.tif\n",
            "S1/Ghana_406026_S1.tif\n",
            "S1/Ghana_53713_S1.tif\n",
            "S1/Ghana_83483_S1.tif\n",
            "S1/Ghana_167233_S1.tif\n",
            "S1/Ghana_141271_S1.tif\n",
            "S1/India_900498_S1.tif\n",
            "S1/India_591317_S1.tif\n",
            "S1/India_747992_S1.tif\n",
            "S1/India_79637_S1.tif\n",
            "S1/India_952728_S1.tif\n",
            "S1/India_828067_S1.tif\n",
            "S1/India_570384_S1.tif\n",
            "S1/India_44475_S1.tif\n",
            "S1/India_80221_S1.tif\n",
            "S1/India_1018327_S1.tif\n",
            "S1/India_592446_S1.tif\n",
            "S1/India_772630_S1.tif\n",
            "S1/India_631692_S1.tif\n",
            "S1/India_399883_S1.tif\n",
            "S1/Mekong_333434_S1.tif\n",
            "S1/Mekong_45934_S1.tif\n",
            "S1/Mekong_1443339_S1.tif\n",
            "S1/Mekong_382276_S1.tif\n",
            "S1/Mekong_254910_S1.tif\n",
            "S1/Mekong_424793_S1.tif\n",
            "S1/Nigeria_417184_S1.tif\n",
            "S1/Nigeria_225131_S1.tif\n",
            "S1/Nigeria_812045_S1.tif\n",
            "S1/Nigeria_22088_S1.tif\n",
            "S1/Pakistan_849790_S1.tif\n",
            "S1/Pakistan_664885_S1.tif\n",
            "S1/Pakistan_694942_S1.tif\n",
            "S1/Pakistan_70625_S1.tif\n",
            "S1/Pakistan_528249_S1.tif\n",
            "S1/Pakistan_167553_S1.tif\n",
            "S1/Paraguay_913449_S1.tif\n",
            "S1/Paraguay_280900_S1.tif\n",
            "S1/Paraguay_790830_S1.tif\n",
            "S1/Paraguay_232281_S1.tif\n",
            "S1/Paraguay_271769_S1.tif\n",
            "S1/Paraguay_40936_S1.tif\n",
            "S1/Paraguay_1029191_S1.tif\n",
            "S1/Paraguay_59731_S1.tif\n",
            "S1/Paraguay_683296_S1.tif\n",
            "S1/Paraguay_34417_S1.tif\n",
            "S1/Paraguay_868895_S1.tif\n",
            "S1/Paraguay_511199_S1.tif\n",
            "S1/Paraguay_80102_S1.tif\n",
            "S1/Paraguay_651904_S1.tif\n",
            "S1/Somalia_699062_S1.tif\n",
            "S1/Somalia_60129_S1.tif\n",
            "S1/Somalia_685158_S1.tif\n",
            "S1/Somalia_166342_S1.tif\n",
            "S1/Somalia_94102_S1.tif\n",
            "S1/Somalia_322855_S1.tif\n",
            "S1/Spain_6860600_S1.tif\n",
            "S1/Spain_7558720_S1.tif\n",
            "S1/Spain_5650136_S1.tif\n",
            "S1/Spain_7370579_S1.tif\n",
            "S1/Spain_6095801_S1.tif\n",
            "S1/Spain_7387658_S1.tif\n",
            "S1/Sri-Lanka_117737_S1.tif\n",
            "S1/Sri-Lanka_1049830_S1.tif\n",
            "S1/Sri-Lanka_534068_S1.tif\n",
            "S1/Sri-Lanka_922192_S1.tif\n",
            "S1/Sri-Lanka_377277_S1.tif\n",
            "S1/Sri-Lanka_649970_S1.tif\n",
            "S1/Sri-Lanka_450918_S1.tif\n",
            "S1/Sri-Lanka_849649_S1.tif\n",
            "S1/Sri-Lanka_713926_S1.tif\n",
            "S1/USA_905409_S1.tif\n",
            "S1/USA_430764_S1.tif\n",
            "S1/USA_350244_S1.tif\n",
            "S1/USA_527077_S1.tif\n",
            "S1/USA_933610_S1.tif\n",
            "S1/USA_66511_S1.tif\n",
            "S1/USA_519181_S1.tif\n",
            "S1/USA_778194_S1.tif\n",
            "S1/USA_67102_S1.tif\n",
            "S1/USA_1049586_S1.tif\n",
            "S1/USA_595451_S1.tif\n",
            "S1/USA_670826_S1.tif\n",
            "S1/USA_504150_S1.tif\n",
            "S1/USA_758178_S1.tif\n",
            "90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-aFQ-Gjw1dy"
      },
      "source": [
        "def ignore_perm_water(all_water, perm_water):\n",
        "  w = all_water.copy()\n",
        "  perm_water = (w == 1) * (perm_water == 1)\n",
        "  w[perm_water] = 255\n",
        "  return w\n",
        "\n",
        "def ignore_flood_water(all_water, perm_water):\n",
        "  w = all_water.copy()\n",
        "  flood_water = (w == 1) * (perm_water == 0)\n",
        "  w[flood_water] = 255\n",
        "  return w\n",
        "\n",
        "def get_flood_flood_test_data(all_water_data, perm_water_data):\n",
        "  flood_flood_test_data = []\n",
        "  for (all_water_d, perm_water_d) in zip(all_water_data, perm_water_data):\n",
        "    # pwy = ignore_perm_water(awy, pwy)\n",
        "    if np.sum(all_water_d[0] == 1) > 0 and np.sum(perm_water_d[0] == 1) == 0:\n",
        "      continue\n",
        "    flood_flood_test_data.append(all_water_d)\n",
        "  return flood_flood_test_data\n",
        "  \n",
        "\n",
        "def get_perm_flood_test_data(all_water_data, perm_water_data):\n",
        "  perm_flood_test_data = []\n",
        "  for (all_water_d, perm_water_d) in zip(all_water_data, perm_water_data):\n",
        "    # pwy = ignore_flood_water(awy, pwy)\n",
        "    if np.sum(all_water_d[0] == 1) > 0 and np.sum(perm_water_d[0] == 1) == 0:\n",
        "      continue\n",
        "    perm_flood_test_data.append(all_water_d)\n",
        "  return perm_flood_test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TQJnTDNxC94"
      },
      "source": [
        "flood_flood_test_data = get_flood_flood_test_data(flood_test_all_data, flood_test_perm_data)\n",
        "perm_flood_test_data = get_perm_flood_test_data(flood_test_all_data, flood_test_perm_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpFyg3nH0yK0",
        "outputId": "875a41b3-ca22-4e4b-9c3f-368a8e2e666c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_all_data = flood_test_all_data\n",
        "test_all_dataset = InMemoryDataset(test_all_data, processTestIm)\n",
        "test_all_loader = torch.utils.data.DataLoader(test_all_dataset, batch_size=1, shuffle=True, sampler=None,\n",
        "                  batch_sampler=None, num_workers=0, collate_fn=lambda x: x[0],\n",
        "                  pin_memory=True, drop_last=False, timeout=0,\n",
        "                  worker_init_fn=None)\n",
        "test_all_iter = iter(test_all_loader)\n",
        "\n",
        "test_flood_data = flood_flood_test_data\n",
        "test_flood_dataset = InMemoryDataset(test_flood_data, processTestIm)\n",
        "test_flood_loader = torch.utils.data.DataLoader(test_flood_dataset, batch_size=1, shuffle=True, sampler=None,\n",
        "                  batch_sampler=None, num_workers=0, collate_fn=lambda x: x[0],\n",
        "                  pin_memory=True, drop_last=False, timeout=0,\n",
        "                  worker_init_fn=None)\n",
        "test_flood_iter = iter(test_flood_loader)\n",
        "\n",
        "valid_data = load_flood_valid_data() #download_perm_valid_data()\n",
        "valid_dataset = InMemoryDataset(valid_data, processTestIm)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=4, shuffle=True, sampler=None,\n",
        "                  batch_sampler=None, num_workers=0, collate_fn=lambda x: (torch.cat([a[0] for a in x], 0), torch.cat([a[1] for a in x], 0)),\n",
        "                  pin_memory=True, drop_last=False, timeout=0,\n",
        "                  worker_init_fn=None)\n",
        "valid_iter = iter(valid_loader)\n",
        "\n",
        "test_perm_data = perm_flood_test_data\n",
        "test_perm_dataset = InMemoryDataset(test_perm_data, processTestIm)\n",
        "test_perm_loader = torch.utils.data.DataLoader(test_perm_dataset, batch_size=1, shuffle=True, sampler=None,\n",
        "                  batch_sampler=None, num_workers=0, collate_fn=lambda x: x[0],\n",
        "                  pin_memory=True, drop_last=False, timeout=0,\n",
        "                  worker_init_fn=None)\n",
        "test_perm_iter = iter(test_perm_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('S1/Ghana_5079_S1.tif', 'QC_v2/Ghana_5079_QC.tif'), ('S1/Ghana_895194_S1.tif', 'QC_v2/Ghana_895194_QC.tif'), ('S1/Ghana_868803_S1.tif', 'QC_v2/Ghana_868803_QC.tif'), ('S1/Ghana_142312_S1.tif', 'QC_v2/Ghana_142312_QC.tif'), ('S1/Ghana_234935_S1.tif', 'QC_v2/Ghana_234935_QC.tif'), ('S1/Ghana_132163_S1.tif', 'QC_v2/Ghana_132163_QC.tif'), ('S1/Ghana_495107_S1.tif', 'QC_v2/Ghana_495107_QC.tif'), ('S1/Ghana_124834_S1.tif', 'QC_v2/Ghana_124834_QC.tif'), ('S1/Ghana_1033830_S1.tif', 'QC_v2/Ghana_1033830_QC.tif'), ('S1/Ghana_277_S1.tif', 'QC_v2/Ghana_277_QC.tif'), ('S1/Ghana_308249_S1.tif', 'QC_v2/Ghana_308249_QC.tif'), ('S1/India_1050276_S1.tif', 'QC_v2/India_1050276_QC.tif'), ('S1/India_764946_S1.tif', 'QC_v2/India_764946_QC.tif'), ('S1/India_118868_S1.tif', 'QC_v2/India_118868_QC.tif'), ('S1/India_533192_S1.tif', 'QC_v2/India_533192_QC.tif'), ('S1/India_180633_S1.tif', 'QC_v2/India_180633_QC.tif'), ('S1/India_244057_S1.tif', 'QC_v2/India_244057_QC.tif'), ('S1/India_691027_S1.tif', 'QC_v2/India_691027_QC.tif'), ('S1/India_769408_S1.tif', 'QC_v2/India_769408_QC.tif'), ('S1/India_1018317_S1.tif', 'QC_v2/India_1018317_QC.tif'), ('S1/India_869358_S1.tif', 'QC_v2/India_869358_QC.tif'), ('S1/India_164336_S1.tif', 'QC_v2/India_164336_QC.tif'), ('S1/India_70352_S1.tif', 'QC_v2/India_70352_QC.tif'), ('S1/India_833266_S1.tif', 'QC_v2/India_833266_QC.tif'), ('S1/India_1068117_S1.tif', 'QC_v2/India_1068117_QC.tif'), ('S1/Mekong_1149855_S1.tif', 'QC_v2/Mekong_1149855_QC.tif'), ('S1/Mekong_977338_S1.tif', 'QC_v2/Mekong_977338_QC.tif'), ('S1/Mekong_474783_S1.tif', 'QC_v2/Mekong_474783_QC.tif'), ('S1/Mekong_293769_S1.tif', 'QC_v2/Mekong_293769_QC.tif'), ('S1/Mekong_1413877_S1.tif', 'QC_v2/Mekong_1413877_QC.tif'), ('S1/Mekong_98310_S1.tif', 'QC_v2/Mekong_98310_QC.tif'), ('S1/Nigeria_31096_S1.tif', 'QC_v2/Nigeria_31096_QC.tif'), ('S1/Nigeria_984831_S1.tif', 'QC_v2/Nigeria_984831_QC.tif'), ('S1/Nigeria_1095404_S1.tif', 'QC_v2/Nigeria_1095404_QC.tif'), ('S1/Nigeria_820924_S1.tif', 'QC_v2/Nigeria_820924_QC.tif'), ('S1/Pakistan_43105_S1.tif', 'QC_v2/Pakistan_43105_QC.tif'), ('S1/Pakistan_94095_S1.tif', 'QC_v2/Pakistan_94095_QC.tif'), ('S1/Pakistan_210595_S1.tif', 'QC_v2/Pakistan_210595_QC.tif'), ('S1/Pakistan_1027214_S1.tif', 'QC_v2/Pakistan_1027214_QC.tif'), ('S1/Pakistan_336228_S1.tif', 'QC_v2/Pakistan_336228_QC.tif'), ('S1/Pakistan_9684_S1.tif', 'QC_v2/Pakistan_9684_QC.tif'), ('S1/Paraguay_305760_S1.tif', 'QC_v2/Paraguay_305760_QC.tif'), ('S1/Paraguay_648632_S1.tif', 'QC_v2/Paraguay_648632_QC.tif'), ('S1/Paraguay_172476_S1.tif', 'QC_v2/Paraguay_172476_QC.tif'), ('S1/Paraguay_581976_S1.tif', 'QC_v2/Paraguay_581976_QC.tif'), ('S1/Paraguay_284928_S1.tif', 'QC_v2/Paraguay_284928_QC.tif'), ('S1/Paraguay_1019808_S1.tif', 'QC_v2/Paraguay_1019808_QC.tif'), ('S1/Paraguay_76868_S1.tif', 'QC_v2/Paraguay_76868_QC.tif'), ('S1/Paraguay_252217_S1.tif', 'QC_v2/Paraguay_252217_QC.tif'), ('S1/Paraguay_205585_S1.tif', 'QC_v2/Paraguay_205585_QC.tif'), ('S1/Paraguay_7894_S1.tif', 'QC_v2/Paraguay_7894_QC.tif'), ('S1/Paraguay_896458_S1.tif', 'QC_v2/Paraguay_896458_QC.tif'), ('S1/Paraguay_657443_S1.tif', 'QC_v2/Paraguay_657443_QC.tif'), ('S1/Paraguay_934240_S1.tif', 'QC_v2/Paraguay_934240_QC.tif'), ('S1/Paraguay_153941_S1.tif', 'QC_v2/Paraguay_153941_QC.tif'), ('S1/Somalia_12849_S1.tif', 'QC_v2/Somalia_12849_QC.tif'), ('S1/Somalia_256539_S1.tif', 'QC_v2/Somalia_256539_QC.tif'), ('S1/Somalia_61368_S1.tif', 'QC_v2/Somalia_61368_QC.tif'), ('S1/Somalia_649376_S1.tif', 'QC_v2/Somalia_649376_QC.tif'), ('S1/Somalia_167787_S1.tif', 'QC_v2/Somalia_167787_QC.tif'), ('S1/Spain_1199913_S1.tif', 'QC_v2/Spain_1199913_QC.tif'), ('S1/Spain_8372658_S1.tif', 'QC_v2/Spain_8372658_QC.tif'), ('S1/Spain_7604243_S1.tif', 'QC_v2/Spain_7604243_QC.tif'), ('S1/Spain_6537196_S1.tif', 'QC_v2/Spain_6537196_QC.tif'), ('S1/Spain_4282030_S1.tif', 'QC_v2/Spain_4282030_QC.tif'), ('S1/Spain_8565131_S1.tif', 'QC_v2/Spain_8565131_QC.tif'), ('S1/Sri-Lanka_85652_S1.tif', 'QC_v2/Sri-Lanka_85652_QC.tif'), ('S1/Sri-Lanka_63307_S1.tif', 'QC_v2/Sri-Lanka_63307_QC.tif'), ('S1/Sri-Lanka_612594_S1.tif', 'QC_v2/Sri-Lanka_612594_QC.tif'), ('S1/Sri-Lanka_132922_S1.tif', 'QC_v2/Sri-Lanka_132922_QC.tif'), ('S1/Sri-Lanka_236030_S1.tif', 'QC_v2/Sri-Lanka_236030_QC.tif'), ('S1/Sri-Lanka_31559_S1.tif', 'QC_v2/Sri-Lanka_31559_QC.tif'), ('S1/Sri-Lanka_236628_S1.tif', 'QC_v2/Sri-Lanka_236628_QC.tif'), ('S1/Sri-Lanka_101973_S1.tif', 'QC_v2/Sri-Lanka_101973_QC.tif'), ('S1/Sri-Lanka_321316_S1.tif', 'QC_v2/Sri-Lanka_321316_QC.tif'), ('S1/USA_826217_S1.tif', 'QC_v2/USA_826217_QC.tif'), ('S1/USA_741073_S1.tif', 'QC_v2/USA_741073_QC.tif'), ('S1/USA_275372_S1.tif', 'QC_v2/USA_275372_QC.tif'), ('S1/USA_19225_S1.tif', 'QC_v2/USA_19225_QC.tif'), ('S1/USA_366607_S1.tif', 'QC_v2/USA_366607_QC.tif'), ('S1/USA_308150_S1.tif', 'QC_v2/USA_308150_QC.tif'), ('S1/USA_1039203_S1.tif', 'QC_v2/USA_1039203_QC.tif'), ('S1/USA_251323_S1.tif', 'QC_v2/USA_251323_QC.tif'), ('S1/USA_1082482_S1.tif', 'QC_v2/USA_1082482_QC.tif'), ('S1/USA_225017_S1.tif', 'QC_v2/USA_225017_QC.tif'), ('S1/USA_986268_S1.tif', 'QC_v2/USA_986268_QC.tif'), ('S1/USA_646878_S1.tif', 'QC_v2/USA_646878_QC.tif'), ('S1/USA_761032_S1.tif', 'QC_v2/USA_761032_QC.tif'), ('S1/USA_741178_S1.tif', 'QC_v2/USA_741178_QC.tif')] files!\n",
            "S1/Ghana_5079_S1.tif\n",
            "0\n",
            "S1/Ghana_5079_S1.tif QC_v2/Ghana_5079_QC.tif\n",
            "S1/Ghana_895194_S1.tif\n",
            "S1/Ghana_868803_S1.tif\n",
            "S1/Ghana_142312_S1.tif\n",
            "S1/Ghana_234935_S1.tif\n",
            "S1/Ghana_132163_S1.tif\n",
            "S1/Ghana_495107_S1.tif\n",
            "S1/Ghana_124834_S1.tif\n",
            "S1/Ghana_1033830_S1.tif\n",
            "S1/Ghana_277_S1.tif\n",
            "S1/Ghana_308249_S1.tif\n",
            "S1/India_1050276_S1.tif\n",
            "S1/India_764946_S1.tif\n",
            "S1/India_118868_S1.tif\n",
            "S1/India_533192_S1.tif\n",
            "S1/India_180633_S1.tif\n",
            "S1/India_244057_S1.tif\n",
            "S1/India_691027_S1.tif\n",
            "S1/India_769408_S1.tif\n",
            "S1/India_1018317_S1.tif\n",
            "S1/India_869358_S1.tif\n",
            "S1/India_164336_S1.tif\n",
            "S1/India_70352_S1.tif\n",
            "S1/India_833266_S1.tif\n",
            "S1/India_1068117_S1.tif\n",
            "S1/Mekong_1149855_S1.tif\n",
            "S1/Mekong_977338_S1.tif\n",
            "S1/Mekong_474783_S1.tif\n",
            "S1/Mekong_293769_S1.tif\n",
            "S1/Mekong_1413877_S1.tif\n",
            "S1/Mekong_98310_S1.tif\n",
            "S1/Nigeria_31096_S1.tif\n",
            "S1/Nigeria_984831_S1.tif\n",
            "S1/Nigeria_1095404_S1.tif\n",
            "S1/Nigeria_820924_S1.tif\n",
            "S1/Pakistan_43105_S1.tif\n",
            "S1/Pakistan_94095_S1.tif\n",
            "S1/Pakistan_210595_S1.tif\n",
            "S1/Pakistan_1027214_S1.tif\n",
            "S1/Pakistan_336228_S1.tif\n",
            "S1/Pakistan_9684_S1.tif\n",
            "S1/Paraguay_305760_S1.tif\n",
            "S1/Paraguay_648632_S1.tif\n",
            "S1/Paraguay_172476_S1.tif\n",
            "S1/Paraguay_581976_S1.tif\n",
            "S1/Paraguay_284928_S1.tif\n",
            "S1/Paraguay_1019808_S1.tif\n",
            "S1/Paraguay_76868_S1.tif\n",
            "S1/Paraguay_252217_S1.tif\n",
            "S1/Paraguay_205585_S1.tif\n",
            "S1/Paraguay_7894_S1.tif\n",
            "S1/Paraguay_896458_S1.tif\n",
            "S1/Paraguay_657443_S1.tif\n",
            "S1/Paraguay_934240_S1.tif\n",
            "S1/Paraguay_153941_S1.tif\n",
            "S1/Somalia_12849_S1.tif\n",
            "S1/Somalia_256539_S1.tif\n",
            "S1/Somalia_61368_S1.tif\n",
            "S1/Somalia_649376_S1.tif\n",
            "S1/Somalia_167787_S1.tif\n",
            "S1/Spain_1199913_S1.tif\n",
            "S1/Spain_8372658_S1.tif\n",
            "S1/Spain_7604243_S1.tif\n",
            "S1/Spain_6537196_S1.tif\n",
            "S1/Spain_4282030_S1.tif\n",
            "S1/Spain_8565131_S1.tif\n",
            "S1/Sri-Lanka_85652_S1.tif\n",
            "S1/Sri-Lanka_63307_S1.tif\n",
            "S1/Sri-Lanka_612594_S1.tif\n",
            "S1/Sri-Lanka_132922_S1.tif\n",
            "S1/Sri-Lanka_236030_S1.tif\n",
            "S1/Sri-Lanka_31559_S1.tif\n",
            "S1/Sri-Lanka_236628_S1.tif\n",
            "S1/Sri-Lanka_101973_S1.tif\n",
            "S1/Sri-Lanka_321316_S1.tif\n",
            "S1/USA_826217_S1.tif\n",
            "S1/USA_741073_S1.tif\n",
            "S1/USA_275372_S1.tif\n",
            "S1/USA_19225_S1.tif\n",
            "S1/USA_366607_S1.tif\n",
            "S1/USA_308150_S1.tif\n",
            "S1/USA_1039203_S1.tif\n",
            "S1/USA_251323_S1.tif\n",
            "S1/USA_1082482_S1.tif\n",
            "S1/USA_225017_S1.tif\n",
            "S1/USA_986268_S1.tif\n",
            "S1/USA_646878_S1.tif\n",
            "S1/USA_761032_S1.tif\n",
            "S1/USA_741178_S1.tif\n",
            "89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OY-W_zkxe4U"
      },
      "source": [
        "# UNet definition\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as TF\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = TF.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet_Model(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet_Model, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCcSmhJ6xh6F"
      },
      "source": [
        "# Refiner network\n",
        "class Refiner_Network(nn.Module):\n",
        "    def __init__(self, seg_model_class, dataset_name, cs_points=None):\n",
        "        super(Refiner_Network, self).__init__()\n",
        "        self.cs_points = cs_points\n",
        "\n",
        "        if dataset_name[:4] == 'sen1':\n",
        "            n_channels = 2\n",
        "        elif dataset_name[:4] == 'sen2':\n",
        "            n_channels = 13\n",
        "        \n",
        "        # assumes that seg_model_class have the same input reqs\n",
        "        self.stage_1  = seg_model_class(n_channels=n_channels, n_classes=2)\n",
        "\n",
        "        if cs_points:\n",
        "            self.stage_2  = seg_model_class(n_channels=n_channels+3, n_classes=2)\n",
        "        else:\n",
        "            self.stage_2  = seg_model_class(n_channels=n_channels+2, n_classes=2)\n",
        "\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        if self.cs_points:\n",
        "            img, cs_points = batch\n",
        "        else:\n",
        "            img = batch\n",
        "\n",
        "        init_pred = self.stage_1(img)  # assume stage_1 is a regular sem seg network\n",
        "\n",
        "        # combine input data and initial_prediction\n",
        "        if self.cs_points:\n",
        "            combined_input = torch.cat((img, init_pred, cs_points), dim=1)  # combine over the channel dim.\n",
        "        else:\n",
        "            combined_input = torch.cat((img, init_pred), dim=1)  # combine over the channel dim.\n",
        "\n",
        "        refined_pred = self.stage_2(combined_input)\n",
        "\n",
        "        return refined_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ValnX5Nixq4f"
      },
      "source": [
        "# get models definition\n",
        "def convertBNtoGN(module, num_groups=16):\n",
        "  if isinstance(module, torch.nn.modules.batchnorm.BatchNorm2d):\n",
        "    return nn.GroupNorm(num_groups, module.num_features,\n",
        "                        eps=module.eps, affine=module.affine)\n",
        "    if module.affine:\n",
        "        mod.weight.data = module.weight.data.clone().detach()\n",
        "        mod.bias.data = module.bias.data.clone().detach()\n",
        "\n",
        "  for name, child in module.named_children():\n",
        "      module.add_module(name, convertBNtoGN(child, num_groups=num_groups))\n",
        "\n",
        "  return module\n",
        "\n",
        "\n",
        "def get_model(model_name, dataset_name, cs_points=None):\n",
        "    model_name = model_name.lower() # handle character case error\n",
        "    dataset_name = dataset_name.lower()\n",
        "\n",
        "    if dataset_name[:4] == 'sen1':\n",
        "        n_channels = 2\n",
        "    elif dataset_name[:4] == 'sen2':\n",
        "        n_channels = 13\n",
        "    else:\n",
        "        print('FATAL: Invalid dataset name \"{}\"'.format(dataset_name))\n",
        "        exit()\n",
        "\n",
        "    if model_name == 'unet':\n",
        "        model = UNet_Model(n_channels=n_channels, n_classes=2)\n",
        "    elif model_name == 'fcn':\n",
        "        from torchvision.models.segmentation import fcn_resnet50\n",
        "        model = fcn_resnet50(pretrained=False, num_classes=2, pretrained_backbone=False)\n",
        "        model.backbone.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3,\n",
        "                                         bias=False)\n",
        "        model = convertBNtoGN(model)\n",
        "    elif model_name == 'refiner':\n",
        "        model = UNet_Model(n_channels=n_channels, n_classes=2)\n",
        "        model = Refiner_Network(UNet_Model, dataset_name, cs_points)\n",
        "    else:\n",
        "        print('FATAL: Invalid model name \"{}\"'.format(model_name))\n",
        "        exit()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khrMBrurzGI-"
      },
      "source": [
        "# net = get_model(MODEL_NAME, 'sen1', crowd_points_path)\n",
        "net = get_model('refiner', 'sen1', crowd_points_path)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "net = net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxUXtIoK0Jqt"
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def computeIOU(output, target):\n",
        "  output = torch.argmax(output, dim=1).flatten() \n",
        "  target = target.flatten()\n",
        "  no_ignore = target.ne(255).cuda()\n",
        "  output = output.masked_select(no_ignore)\n",
        "  target = target.masked_select(no_ignore)\n",
        "  intersection = torch.sum(output * target)\n",
        "  union = torch.sum(target) + torch.sum(output) - intersection\n",
        "  iou = (intersection + .0000001) / (union + .0000001)\n",
        "  if iou != iou:\n",
        "    print(\"failed, replacing with 0\")\n",
        "    iou = torch.tensor(0).float()\n",
        "  return iou\n",
        "  \n",
        "\n",
        "def computeAccuracy(output, target):\n",
        "  output = torch.argmax(output, dim=1).flatten() \n",
        "  target = target.flatten()\n",
        "  no_ignore = target.ne(255).cuda()\n",
        "  output = output.masked_select(no_ignore)\n",
        "  target = target.masked_select(no_ignore)\n",
        "  correct = torch.sum(output.eq(target))\n",
        "  return correct.float() / len(target)\n",
        "  \n",
        "def truePositives(output, target):\n",
        "  output = torch.argmax(output, dim=1).flatten() \n",
        "  target = target.flatten()\n",
        "  no_ignore = target.ne(255).cuda()\n",
        "  output = output.masked_select(no_ignore)\n",
        "  target = target.masked_select(no_ignore)\n",
        "  correct = torch.sum(output * target)\n",
        "  return correct\n",
        "\n",
        "def trueNegatives(output, target):\n",
        "  output = torch.argmax(output, dim=1).flatten() \n",
        "  target = target.flatten()\n",
        "  no_ignore = target.ne(255).cuda()\n",
        "  output = output.masked_select(no_ignore)\n",
        "  target = target.masked_select(no_ignore)\n",
        "  output = (output == 0)\n",
        "  target = (target == 0)\n",
        "  correct = torch.sum(output * target)\n",
        "  return correct\n",
        "\n",
        "def falsePositives(output, target):\n",
        "  output = torch.argmax(output, dim=1).flatten() \n",
        "  target = target.flatten()\n",
        "  no_ignore = target.ne(255).cuda()\n",
        "  output = output.masked_select(no_ignore)\n",
        "  target = target.masked_select(no_ignore)\n",
        "  output = (output == 1)\n",
        "  target = (target == 0)\n",
        "  correct = torch.sum(output * target)\n",
        "  return correct\n",
        "\n",
        "def falseNegatives(output, target):\n",
        "  output = torch.argmax(output, dim=1).flatten() \n",
        "  target = target.flatten()\n",
        "  no_ignore = target.ne(255).cuda()\n",
        "  output = output.masked_select(no_ignore)\n",
        "  target = target.masked_select(no_ignore)\n",
        "  output = (output == 0)\n",
        "  target = (target == 1)\n",
        "  correct = torch.sum(output * target)\n",
        "  return correct\n",
        "\n",
        "training_losses = []\n",
        "training_accuracies = []\n",
        "training_ious = []\n",
        "valid_losses = []\n",
        "valid_accuracies = []\n",
        "valid_ious = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGX3P9LFjwJc"
      },
      "source": [
        "def train_loop(inputs, labels, net, optimizer, scheduler):\n",
        "  global running_loss\n",
        "  global running_iou\n",
        "  global running_count\n",
        "  global running_accuracy\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "  net = net.cuda()\n",
        "  # forward + backward + optimize\n",
        "  outputs = net(inputs.cuda())\n",
        "  loss = criterion(outputs[\"out\"], labels.long().cuda())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  scheduler.step()\n",
        "\n",
        "  running_loss += loss\n",
        "  running_iou += computeIOU(outputs[\"out\"], labels.cuda())\n",
        "  running_accuracy += computeAccuracy(outputs[\"out\"], labels.cuda())\n",
        "  running_count += 1\n",
        "\n",
        "def validation_loop(validation_data_loader, net):\n",
        "  global running_loss\n",
        "  global running_iou\n",
        "  global running_count\n",
        "  global running_accuracy\n",
        "  global max_valid_iou\n",
        "\n",
        "  global training_losses\n",
        "  global training_accuracies\n",
        "  global training_ious\n",
        "  global valid_losses\n",
        "  global valid_accuracies\n",
        "  global valid_ious\n",
        "\n",
        "  net = net.eval()\n",
        "  net = net.cuda()\n",
        "  count = 0\n",
        "  iou = 0\n",
        "  loss = 0\n",
        "  accuracy = 0\n",
        "  with torch.no_grad():\n",
        "      for batch in validation_data_loader:\n",
        "          images = batch['image']\n",
        "          labels = batch['label']\n",
        "          if crowd_points_path is None:\n",
        "            outputs = net(images.cuda())\n",
        "          else:\n",
        "            point_img = batch['point_img']\n",
        "            outputs = net([images.cuda(), point_img.cuda()])\n",
        "          if type(outputs) is dict:\n",
        "            outputs = outputs['out']\n",
        "          valid_loss = criterion(outputs, labels.long().cuda())\n",
        "          valid_iou = computeIOU(outputs, labels.cuda())\n",
        "          valid_accuracy = computeAccuracy(outputs, labels.cuda())\n",
        "          iou += valid_iou\n",
        "          loss += valid_loss\n",
        "          accuracy += valid_accuracy\n",
        "          count += 1\n",
        "\n",
        "  iou = iou / count\n",
        "  accuracy = accuracy / count\n",
        "\n",
        "  if iou > max_valid_iou:\n",
        "    max_valid_iou = iou\n",
        "    save_path = os.path.join(\"checkpoints\", \"{}_{}_{}.cp\".format(RUNNAME, i, iou.item()))\n",
        "    torch.save(net.state_dict(), save_path)\n",
        "    print(\"model saved at\", save_path)\n",
        "\n",
        "  loss = loss / count\n",
        "  print(\"Training Loss:\", running_loss / running_count)\n",
        "  print(\"Training IOU:\", running_iou / running_count)\n",
        "  print(\"Training Accuracy:\", running_accuracy / running_count)\n",
        "  print(\"Validation Loss:\", loss)\n",
        "  print(\"Validation IOU:\", iou)\n",
        "  print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "  training_losses.append(running_loss / running_count)\n",
        "  training_accuracies.append(running_accuracy / running_count)\n",
        "  training_ious.append(running_iou / running_count)\n",
        "  valid_losses.append(loss)\n",
        "  valid_accuracies.append(accuracy)\n",
        "  valid_ious.append(iou)\n",
        "\n",
        "def test_loop(test_data_loader, net):\n",
        "  net = net.eval()\n",
        "  # net = net.cuda()\n",
        "  count = 0\n",
        "  iou = 0\n",
        "  loss = 0\n",
        "  accuracy = 0\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  tn = 0\n",
        "  fn = 0\n",
        "  with torch.no_grad():\n",
        "      for batch in tqdm(test_data_loader):\n",
        "          images = batch['image']\n",
        "          labels = batch['label']\n",
        "\n",
        "          if crowd_points_path is None:\n",
        "            outputs = net(images.cuda())\n",
        "          else:\n",
        "            point_img = batch['point_img']\n",
        "            outputs = net([images.cuda(), point_img.cuda()])\n",
        "\n",
        "          if type(outputs) is dict:\n",
        "            outputs = outputs['out']\n",
        "          valid_iou = computeIOU(outputs, labels.cuda())\n",
        "          iou += valid_iou\n",
        "          accuracy += computeAccuracy(outputs, labels.cuda())\n",
        "          tp +=  truePositives(outputs, labels.cuda())\n",
        "          fp +=  falsePositives(outputs, labels.cuda())\n",
        "          tn +=  trueNegatives(outputs, labels.cuda())\n",
        "          fn +=  falseNegatives(outputs, labels.cuda())\n",
        "          count += 1\n",
        "\n",
        "  iou = iou / count\n",
        "  print(\"Test Mean IOU:\", iou)\n",
        "  print(\"Total IOU:\", (tp.float() / (fn + fp + tp)))\n",
        "  print(\"OMISSON:\", fn.float() / (fn + tp))\n",
        "  print(\"COMMISSON:\", fp.float() / (tn + fp))\n",
        "  print(\"Test Accuracy:\", accuracy / count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVvcrc7og4mR"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "running_loss = 0\n",
        "running_iou = 0\n",
        "running_count = 0\n",
        "running_accuracy = 0\n",
        "\n",
        "training_losses = []\n",
        "training_accuracies = []\n",
        "training_ious = []\n",
        "valid_losses = []\n",
        "valid_accuracies = []\n",
        "valid_ious = []\n",
        "\n",
        "\n",
        "def train_epoch(net, optimizer, scheduler, train_iter):\n",
        "  for (inputs, labels) in tqdm(train_iter):\n",
        "    train(inputs.cuda(), labels.cuda(), net.cuda(), optimizer, scheduler)\n",
        " \n",
        "\n",
        "def train_validation_loop(net, optimizer, scheduler, train_loader,\n",
        "                          valid_loader, num_epochs, cur_epoch):\n",
        "  global running_loss\n",
        "  global running_iou\n",
        "  global running_count\n",
        "  global running_accuracy\n",
        "  net = net.train()\n",
        "  running_loss = 0\n",
        "  running_iou = 0\n",
        "  running_count = 0\n",
        "  running_accuracy = 0\n",
        "  for i in tqdm(range(num_epochs)):\n",
        "    train_iter = iter(train_loader)\n",
        "    train_epoch(net, optimizer, scheduler, train_iter)\n",
        "  clear_output()\n",
        "  print(\"Current Epoch:\", cur_epoch)\n",
        "  validation_loop(iter(valid_loader), net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftPSv-0P0pTP",
        "outputId": "1e06dfe2-ea1e-4a47-deb2-601319bf0fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383,
          "referenced_widgets": [
            "7db1ba0d1b3f46a480cb8ab4cc9aab71",
            "1db6ecfb72054e4eb7288600fbc46f62",
            "53ea9700fa0e41b9a39d94f383aa6252",
            "7c40911347034a9b94d43bc8c62673f7",
            "d66b8e660df84820af4806d4991e68d1",
            "5c8c5213ce8f4b5aa3796eee88cc11f1",
            "784fd88d1c6d457685c8353f48e58041",
            "efdc15e854f744ec9a6fa7abd5fffd45"
          ]
        }
      },
      "source": [
        "import torch \n",
        "\n",
        "test_loop(test_all_loader, net)\n",
        "test_loop(test_flood_loader, net)\n",
        "test_loop(test_perm_loader, net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7db1ba0d1b3f46a480cb8ab4cc9aab71",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=90.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-182-ba85d4d522e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_all_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_flood_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_perm_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-180-269352a52823>\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(test_data_loader, net)\u001b[0m\n\u001b[1;32m    107\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m           \u001b[0mvalid_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeIOU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m           \u001b[0miou\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalid_iou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m           \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcomputeAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUmY3Xoig-_1",
        "outputId": "f5b57bd0-02fe-4048-8595-8a7900c50176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "import os\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "max_valid_iou = 0\n",
        "start = 0\n",
        "\n",
        "epochs = []\n",
        "training_losses = []\n",
        "training_accuracies = []\n",
        "training_ious = []\n",
        "valid_losses = []\n",
        "valid_accuracies = []\n",
        "valid_ious = []\n",
        "\n",
        "for i in range(start, 1000):\n",
        "  train_validation_loop(net, optimizer, scheduler, train_loader, valid_loader, 10, i)\n",
        "  epochs.append(i)\n",
        "  x = epochs\n",
        "  plt.plot(x, training_losses, label='training losses')\n",
        "  plt.plot(x, training_accuracies, 'tab:orange', label='training accuracy')\n",
        "  plt.plot(x, training_ious, 'tab:purple', label='training iou')\n",
        "  plt.plot(x, valid_losses, label='valid losses')\n",
        "  plt.plot(x, valid_accuracies, 'tab:red',label='valid accuracy')\n",
        "  plt.plot(x, valid_ious, 'tab:green',label='valid iou')\n",
        "  plt.legend(loc=\"upper left\")\n",
        "\n",
        "  display(plt.show())\n",
        "\n",
        "  print(\"max valid iou:\", max_valid_iou)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-164-8365ceef919c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mtrain_validation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY0rshkBfpLe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}